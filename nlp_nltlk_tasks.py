# -*- coding: utf-8 -*-
"""NLP_nltlk_tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TiF21HZPHPA3YrPFCJe3cHtTFIqc2I8j

#TASK 1:

#A.
"""

import nltk
nltk.download('gutenberg')
nltk.corpus.gutenberg.fileids()

"""#B."""

from nltk import word_tokenize, sent_tokenize
nltk.download('punkt')  # required for tokenizing
#C275

from nltk.corpus import gutenberg

def analyze_gutenberg_texts():
    for fileid in gutenberg.fileids():
        raw_text = gutenberg.raw(fileid)
        words = gutenberg.words(fileid)
        sents = gutenberg.sents(fileid)

        total_words = len(words)
        total_chars = len(raw_text)
        total_sents = len(sents)
        avg_chars_per_word = total_chars / total_words
        ttr = len(set(words)) / total_words  # Type-Token Ratio

        print(f"\nAnalysis for: {fileid}")
        print(f"Total Words: {total_words}")
        print(f"Total Characters: {total_chars}")
        print(f"Total Sentences: {total_sents}")
        print(f"Average Characters per Word: {avg_chars_per_word:.2f}")
        print(f"Type Token Ratio (TTR): {ttr:.3f}")

analyze_gutenberg_texts()

"""#C"""

nltk.download('brown')
from nltk.corpus import brown
brown.categories()#C275

"""#D"""

import nltk
from nltk.corpus import brown
#C275
categories = brown.categories()
print(categories)

"""a."""

def compare_categories():
    categories = brown.categories()
    print("\nTotal Words in Each Brown Corpus Category:")
    for category in categories:
        words_in_light = brown.words(categories=category)
        print(f"{category:20} : {len(words_in_light)}")
#C275
compare_categories()

from nltk.corpus import brown
#C275
categories = brown.categories()
#C275
print("\nTotal Sentences in Each Brown Corpus Category:")
for category in categories:
    sentences_in_category = brown.sents(categories=category)
    print(f"{category:20} : {len(sentences_in_category)}")

"""e."""

import urllib.request
#C275
# Accessing a text file from the web (Project Gutenberg example)
url = "https://www.gutenberg.org/files/1342/1342-0.txt"  # Pride and Prejudice
response = urllib.request.urlopen(url)
web_text = response.read().decode('utf-8')

# taking 800 characters
print(web_text[:800])

"""f. (a)"""

tokens = word_tokenize(web_text)#C275
print("Total tokens:", len(tokens))
print("First 20 tokens:", tokens[:20])

"""(b)

(c)
"""

from nltk.text import Text
import nltk#C275
nltk.download('stopwords')
nltk_text = Text(tokens)
print("Top 10 collocations:")
nltk_text.collocations()

nltk_text.concordance("Darcy")#C275

"""g."""

pip install beautifulsoup4

"""h."""

import urllib.request
from bs4 import BeautifulSoup
#C275
# Load an HTML webpage
url = "https://en.wikipedia.org/wiki/Natural_language_processing"
response = urllib.request.urlopen(url)
#C275
# Parse HTML
soup = BeautifulSoup(response, 'html.parser')
html_text = soup.get_text()

# Print first 500 characters
print(html_text[:500])

"""h.(a)"""

# word tokenization
from nltk.tokenize import word_tokenize

html_tokens = word_tokenize(html_text)
print("First 20 tokens:", html_tokens[:20])
#C275

"""h.(b)"""

#Concordance
nltk_text = Text(html_tokens)
nltk_text.concordance("natural")
#C275

"""i."""

# Open and read local file
with open("c275.txt", "r", encoding="utf-8") as file:
    local_text = file.read()#C275

print(local_text[:500])  # preview first 500 characters

local_tokens = word_tokenize(local_text)#C275
print("First 20 tokens:", local_tokens[:20])

pip install wordcloud matplotlib

from wordcloud import WordCloud
import matplotlib.pyplot as plt
#C275
sample_text = "NLP is fun. NLP is powerful. NLP stands for Natural Language Processing."

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(sample_text)
#C275
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Sample Word Cloud")
plt.show()